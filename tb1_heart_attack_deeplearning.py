# -*- coding: utf-8 -*-
"""TB1_Heart_Attack_Deeplearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mg9bHLyV9Ln0bKvvoSoalF9CnsdtRSd4
"""

# Mengimport library yang dibutuhkan
import torch
import warnings
import seaborn as sns
import numpy as np 
import pandas as pd
import tensorflow as tf
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split 
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')

# Upload file dataset
from google.colab import files
uploaded = files.upload()

# Membaca file heart.csv
data = pd.read_csv('heart.csv')

# melihat informasi dataset pada 5 baris pertama
data.head()

# Menampilkan data deskriptif
data.describe()

# Melakukan pengecekan terhadap data null
data.isnull().sum()

# Menghitung jumlah value label 
data['target'].value_counts()

# Grafik perbandingan data label pasien positive (1) dan negative (0) 
sns.countplot(x="target", data=data, palette="bwr")
plt.show()

# Menghitung persentase jumlah pasien yang terjangkit penyakit jantung dengan yang tidak
countNoDisease = len(data[data.target == 0])
countHaveDisease = len(data[data.target == 1])
print("Percentage of Patients Haven't Heart Disease: {:.2f}%".format((countNoDisease / (len(data.target))*100)))
print("Percentage of Patients Have Heart Disease: {:.2f}%".format((countHaveDisease / (len(data.target))*100)))

# Grafik perbanding jumlah pasien berdasarkan kelamin
sns.countplot(x='sex', data=data, palette="mako_r")
plt.xlabel("Sex (0 = female, 1= male)")
plt.show()

# Menhitung persentase jumlah pasien berdasarkan kelamin
countFemale = len(data[data.sex == 0])
countMale = len(data[data.sex == 1])
print("Percentage of Female Patients: {:.2f}%".format((countFemale / (len(data.sex))*100)))
print("Percentage of Male Patients: {:.2f}%".format((countMale / (len(data.sex))*100)))

# Grafik perbandingan pasien terkena penyakit jantung berdasarkan usia
pd.crosstab(data.age,data.target).plot(kind="bar",figsize=(20,6))
plt.title('Heart Disease Frequency for Ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.savefig('heartDiseaseAndAges.png')
plt.show()

# Grafik perbandingan jumlah pasien terjangkit dan yang tidak berdasarkan kelamin
pd.crosstab(data.sex,data.target).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111' ])
plt.title('Heart Disease Frequency for Sex')
plt.xlabel('Sex (0 = Female, 1 = Male)')
plt.xticks(rotation=0)
plt.legend(["Haven't Disease", "Have Disease"])
plt.ylabel('Frequency')
plt.show()

plt.scatter(x=data.age[data.target==1], y=data.thalach[(data.target==1)], c="red")
plt.scatter(x=data.age[data.target==0], y=data.thalach[(data.target==0)])
plt.legend(["Disease", "Not Disease"])
plt.xlabel("Age")
plt.ylabel("Maximum Heart Rate")
plt.show()

# Grafik tingkat gula darah pasien terjangkit dengan yang tidak
pd.crosstab(data.fbs,data.target).plot(kind="bar",figsize=(15,6),color=['#FFC300','#581845' ])
plt.title('Heart Disease Frequency According To FBS')
plt.xlabel('FBS - (Fasting Blood Sugar > 120 mg/dl) (1 = true; 0 = false)')
plt.xticks(rotation = 0)
plt.legend(["Haven't Disease", "Have Disease"])
plt.ylabel('Frequency of Disease or Not')
plt.show()

# memisahkan atribut dan label
X = data.drop(["target"], axis=1)
X[0:5]

Y = data["target"].values
Y[0:5]

# split the dataset into the training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=1)

# Untuk mengetahui panjang/jumldah data pada x_train , x_test, y_train, y_test
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

# feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# RNN MODELS"""

# create the model
model = torch.nn.Linear(X_train.shape[1], 1)

# load sets in format compatible with pytorch
X_train = torch.from_numpy(X_train.astype(np.float32))
X_test = torch.from_numpy(X_test.astype(np.float32))

y_train = torch.from_numpy(y_train).float().reshape(-1, 1)
y_test = torch.from_numpy(y_test).float().reshape(-1, 1)

def configure_loss_function():
    return torch.nn.BCEWithLogitsLoss()

# use Adam optimiser for gradient descent
def configure_optimizer(model):
    return torch.optim.Adam(model.parameters(), lr = 0.001)

# define the loss function to compare the output with the target
criterion = configure_loss_function()
optimizer = configure_optimizer(model)

# run the model
epochs = 5000
# initialise the train_loss & test_losses which will be updated
train_losses = np.zeros(epochs)
test_losses = np.zeros(epochs)

for epoch in range(epochs): 
    y_pred = model(X_train)
    loss = criterion(y_pred, y_train)
    # clear old gradients from the last step
    optimizer.zero_grad()
    # compute the gradients necessary to adjust the weights
    loss.backward()
    # update the weights of the neural network
    optimizer.step()

    outputs_test = model(X_test)
    loss_test = criterion(outputs_test, y_test)

    train_losses[epoch] = loss.item()
    test_losses[epoch] = loss_test.item()

    if (epoch + 1) % 50 == 0:
      print (str('Epoch ') + str((epoch+1)) + str('/') + str(epochs) + str(',  training loss = ') + 
             str((loss.item())) + str(', test loss = ') + str(loss_test.item()))

# visualise the test and train loss
plt.plot(train_losses, label = 'train loss')
plt.plot(test_losses, label = 'test loss')
plt.legend()
plt.title('Model Loss')

with torch.no_grad():
  output_train = model(X_train)
  output_train = (output_train.numpy() > 0)

  train_acc = np.mean(y_train.numpy() == output_train)

  output_test = model(X_test)
  output_test = (output_test.numpy() > 0)
  
  test_acc = np.mean(y_test.numpy() == output_test)

print ('Train accuracy is: ' + str(train_acc))
print ('Test accuracy is: ' + str(test_acc))

"""# LSTM MODELS"""

# split the dataset into the training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=1)

#membuat model LSTM
from keras import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout

embedding_size=32
max_words=5000

predLSTN = Sequential()
predLSTN.add(Embedding(max_words, embedding_size, input_length=X_train.shape[1]))
predLSTN.add(LSTM(100))
predLSTN.add(Dense(2,activation='softmax'))

print(predLSTN.summary())

#melalukan compile model dengan loss, optimizer, dan metrics
predLSTN.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Mengubah categorical variable pada y_train menjadi variable numerik
y_train_dummies = pd.get_dummies(y_train).values
print('Shape of Label tensor: ', y_train_dummies.shape)

# Mengubah categorical variable pada y_test menjadi variable numerik
y_test_dummies = pd.get_dummies(y_test).values
print('Shape of Label tensor: ', y_test_dummies.shape)

#train the model
predLSTN.fit(X_train, y_train_dummies, epochs=10, batch_size=22)

predLSTN.save('HeartDisease')

# model evaluation
from keras.models import load_model

predLSTN = load_model('HeartDisease')
scores = predLSTN.evaluate(X_test, y_test_dummies)

LSTM_accuracy = scores[1]*100

print('Test accuracy: ', scores[1]*100, '%')

"""# CNN MODELS"""

# Melakukan pembagian data menjadi 2 bagian (data train & data test)
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=5)

# Melakukan proses standarisasi
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)

# Reshaping untuk variabel x_train dan x_test

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

from keras.optimizers import Adam
from keras.layers.convolutional import Conv1D  
from keras.layers import Dense, Activation, Flatten

# Membuat jaringan syarat tiruan untuk model CNN
model = Sequential()
 
# Menambah layer CNN yang pertama dan Dropout regularisation
model.add(Conv1D(filters=150, kernel_size=5, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dropout(0.2))
 
# Menambah layer CNN yang kedua dan Dropout regularisation
model.add(Conv1D(filters=100, kernel_size=5, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dropout(0.2))
 
# Menambah layer CNN yang ketiga dan Dropout regularisation
model.add(Conv1D(filters=50, kernel_size=5, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dropout(0.2))
model.add(Flatten())

# Menambah layer output dan dropout regularisation
model.add(Dense(units = 1))

# Melihat rancangan network CNN yang telah dibuat
model.summary()

opt = Adam(lr=.001)
 
# Compile RNN dengan nilai opt, loss, dan matrics
model.compile(optimizer =opt, loss = 'mean_squared_error', metrics=['accuracy'])

# Membuat model jaringan syaraf tiruan untuk model CNN
history = model.fit(x_train, y_train, epochs = 40, batch_size = 200, validation_data=(x_test, y_test))

# Melakukan prediksi untuk variable test
y_pred_test = model.predict(x_test)
y_pred_test

from sklearn.metrics import confusion_matrix
tp, tn, fp, fn = confusion_matrix(y_test, y_pred_test.round()).ravel()
print(tp, tn, fp, fn)

# Melihat tingkat akurasi pada variable test
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_test.round()))

